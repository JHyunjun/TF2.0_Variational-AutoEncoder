{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2.0_VAE_MNIST.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqwcNE0YuYSyFInNdNAPIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/TF2.0_Variational-AutoEncoder/blob/main/TF2_0_VAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQHcWKi_H-uG"
      },
      "outputs": [],
      "source": [
        "# Function call\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import Input, Dense, Lambda, Reshape\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from keras.datasets import mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper Parameters\n",
        "batch_size = 16\n",
        "original_dim = 784 #MNIST\n",
        "latent_dim = 2\n",
        "intermediate_dim = 256\n",
        "epochs = 10\n",
        "epsilon_std = 1.0 #For Sampling"
      ],
      "metadata": {
        "id": "m0d9SBwPII-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(args: tuple):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
        "                              stddev=epsilon_std)\n",
        "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "  #Mean과 Log_Std로 Sampling한 최종 결과물 Return"
      ],
      "metadata": {
        "id": "6Rk0Yd9sIYXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Designing Encoder\n",
        "x = Input(shape=(original_dim,), name=\"input\")\n",
        "h1 = Dense(intermediate_dim, activation='relu', name=\"encoding_1\")(x)\n",
        "h2 = Dense(int(intermediate_dim)/2, activation='relu', name=\"encoding_2\")(h1)\n",
        "h = Dense(int(intermediate_dim)/2, activation='relu', name=\"encoding_3\")(h2)\n",
        "z_mean = Dense(latent_dim, name=\"mean\")(h)\n",
        "z_log_var = Dense(latent_dim, name=\"log-variance\")(h)\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "encoder = Model(x, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "UavXHcYGImxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Designing Decoder\n",
        "input_decoder = Input(shape=(latent_dim,), name=\"decoder_input\")\n",
        "decoder_h1 = Dense(int(intermediate_dim/2), activation='relu', name=\"decoder_h1\")(input_decoder)\n",
        "decoder_h2 = Dense(intermediate_dim, activation='relu', name=\"decoder_h2\")(decoder_h1)\n",
        "decoder_h = Dense(int(intermediate_dim*1.5), activation='relu', name=\"decoder_h3\")(decoder_h2)\n",
        "x_decoded = Dense(original_dim, activation='sigmoid', name=\"flat_decoded\")(decoder_h)\n",
        "decoder = Model(input_decoder, x_decoded, name=\"decoder\") #From size of Latent space to Original image\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "FFDCanoCKM_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_combined = decoder(encoder(x)[2]) #Encoder의 3번째 Return값[2]이 Z[z_mean, z_log_var, z]\n",
        "vae = Model(x, output_combined)\n",
        "vae.summary()"
      ],
      "metadata": {
        "id": "tChwqzy6N-0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Function : KL-Divergence\n",
        "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "vae.add_loss(K.mean(kl_loss) / 784.)\n",
        "vae.compile(optimizer='Adam', loss=\"binary_crossentropy\")\n",
        "vae.summary()"
      ],
      "metadata": {
        "id": "NOkikqkrOSkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.#Norm\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
      ],
      "metadata": {
        "id": "wjm4tmykPn_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "vae.fit(x_train, x_train,shuffle=False,epochs=epochs,batch_size=batch_size)"
      ],
      "metadata": {
        "id": "5AGgPhQAProI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2D-Plotting을 위해선 Latent space를 2-Dimension으로밖에 잡을 수 없다는 한계점 존재\n",
        "n = 1\n",
        "m = 5\n",
        "digit_size = 28 #MNIST default size\n",
        "figure = np.zeros((digit_size * n, digit_size * m))\n",
        "grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) #linspace로 0~1사이 10등분하고 이걸 확률로본 다음, norm.ppf를 통해 Gaussian에서 각각 10개 값에 대한 pdf를 읽어옴\n",
        "grid_y = norm.ppf(np.linspace(0.05, 0.95, m)) #여기서 읽어온 pdf값을 Mean, std로 가정하고 Decode진행\n",
        "\n",
        "for i, yi in enumerate(grid_x): #for a,b in enumerate는 정수 a와 grid_x의 b값을 매칭해주는것. ex) (0,-1.64444)\n",
        "    for j, xi in enumerate(grid_y):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        print(\"xi : \",xi, \"yi : \",yi)\n",
        "        print(\"i : \",i, \"j : \",j)\n",
        "        x_decoded = decoder.predict(z_sample)\n",
        "        print(i,\" Decode : \",x_decoded.shape)\n",
        "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "               j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='Greys_r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NkLTHFwGQ0cx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}